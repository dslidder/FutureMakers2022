# FutureMakers2022

7/6 (Day 1)
Learned about the general structure of Futuremakers colab notebooks
Learned about numpy and pandas.
Was a bit more challenging than I expected-- I'm actually quite rusty with Python!-- but I enjoyed having to figure things out instead of copying and pasting code or being told exactly what to do

7/7 (Day 2)
Learned about statistical machine learning algorithms: decision tree and random forest
Was cool to be able to visualize the clustering with numpy and matplotlib. Also pretty interesting to look at the graphs and come to the realization that a random forest is just a group of decision trees made with different features that take a majority vote on the final classification. 

7/8 (Day 3)
Learned about clustering, perceptrons and linear regression models and got introduced to Tensorflow's neural networks.
Interesting to find out the problems with linearity and realize that many classification problems need a more complex approach than a simple line to divide inputs into classes. It was also fun to play around with hyperparameters like number of nodes in each layer and epochs to get the model to various accuracies. 

7/12 (Day 4)
After the three day break it was a pretty fun jump back into machine learning to get to play with tensorflow/Keras
The first part was learning about tensors themselves (along with vectors and scalars). Was pretty easy to understand, and neat to be introduced to a concept that I know also applies to physics. We then preprocessed data, which I learned is a very important step to reduce noise and make it possible for the model to train.
From what I've seen today, Keras is pretty easy and straightforward- it only takes a line of code to define each layer, it's activation function and its size, as well as the optimizer and loss function for the model. Training and fitting the model is also very streamlined in the same way.

7/13 (Day 5)
Wow, this was a giant step up from Day 4. Tensorflow wrapped up all the processes that go into creating a model in such a nice package (not literally!) that it was easy to forget how much math and conceputal understanding goes into a neural network! I struggled a lot with this notebook-- the questions themselves were fairly straightforward, but to really soak in what the optimizers do in a model, what role back propagation plays, and the exact calculations behind all of them was difficult for me. Despite this, I think I have gained an incredible amout of valuable information from today's notebook that many other programs and websites tend to gloss over.

7/15 (Day 7)
I already had an idea of how important tackling bias in machine learning was- I've seen quite a few articles in the past couple years, however it was interesting to experience the fact that even when I was aware, and trying to be unbiased, circumstances and external factors still added bias into my choices, and was reflected in the models. 
