# FutureMakers2022

7/6 (Day 1)
Learned about the general structure of Futuremakers colab notebooks
Learned about numpy and pandas.
Was a bit more challenging than I expected-- I'm actually quite rusty with Python!-- but I enjoyed having to figure things out instead of copying and pasting code or being told exactly what to do

7/7 (Day 2)
Learned about statistical machine learning algorithms: decision tree and random forest
Was cool to be able to visualize the clustering with numpy and matplotlib. Also pretty interesting to look at the graphs and come to the realization that a random forest is just a group of decision trees made with different features that take a majority vote on the final classification. 

7/8 (Day 3)
Learned about clustering, perceptrons and linear regression models and got introduced to Tensorflow's neural networks.
Interesting to find out the problems with linearity and realize that many classification problems need a more complex approach than a simple line to divide inputs into classes. It was also fun to play around with hyperparameters like number of nodes in each layer and epochs to get the model to various accuracies. 

7/12 (Day 4)
[Intro to TF]
After the three day break it was a pretty fun jump back into machine learning to get to play with tensorflow/Keras
The first part was learning about tensors themselves (along with vectors and scalars). Was pretty easy to understand, and neat to be introduced to a concept that I know also applies to physics. We then preprocessed data, which I learned is a very important step to reduce noise and make it possible for the model to train.
From what I've seen today, Keras is pretty easy and straightforward- it only takes a line of code to define each layer, it's activation function and its size, as well as the optimizer and loss function for the model. Training and fitting the model is also very streamlined in the same way.

7/13 (Day 5)
['Handwritten' Neural Nets]
Wow, this was a giant step up from Day 4. Tensorflow wrapped up all the processes that go into creating a model in such a nice package (not literally!) that it was easy to forget how much math and conceputal understanding goes into a neural network! I struggled a lot with this notebook-- the questions themselves were fairly straightforward, but to really soak in what the optimizers do in a model, what role back propagation plays, and the exact calculations behind all of them was difficult for me. Despite this, I think I have gained an incredible amout of valuable information from today's notebook that many other programs and websites tend to gloss over.

7/14 (Day 6)
[CNN Intro]

7/15 (Day 7)
[Ethics Pt. 1]
I already had an idea of how important tackling bias in machine learning was- I've seen quite a few articles in the past couple years, however it was interesting to experience the fact that even when I was aware, and trying to be unbiased, circumstances and external factors still added bias into my choices, and was reflected in the models. 

7/18 (Day 8)
[CNN Implementation]
Wow! This stuff is so cool! It ws amazing to be able to actually see the filtered, pooled feature maps in between Conv layers- I was really getting a clear picture and strong insight into how these models really work. Also, the Cifar100 is pretty hard!

7/19 (Day 9)
[Loss Functions]
Today's notebook really gave me an important insight into loss functions. From what I've seen so far, I gather that loss calculation and back propagation are the most important aspects of a self-learning model, and despite my prior knowledge in other regions, I've always been a bit fuzzy on this step. Learning exactly how the predictions were compared to the true values was pivotal in my understanding of the mathematical underpinnings of a model, and being able to code it myself really drove the point home.

7/20 (Day 10)
[Activation Functions]
ReLU strikes again! While I enjoyed the relatively more subdued difficulty level of this lesson, I still really liked the do-it-yourself aspect of today's notebook. Just like yesterday, I really found it helpful to be coding these black-box seeming functions myself so I could understand how they, and by extension the models which use them, work. It was also really interesting to see how some graphs that I'd learned about in math class like sigmoid and tanh could apply to machine learning, and the pros and pitfalls of each of these methods.

7/21 (Day 11)
[Ethics Pt. 2]
It was pretty neat to learn about all the ethical thought that goes into building a machine learning model and implementing it in real life. I knew there were a lot of ethical concerns about AI: a good resource is the book "Weapons of Math Destruction" by Cathy O'Neil, but I really found it eye-opening to visit this from an inside perspective as someone who is now actually learning about making these models.
